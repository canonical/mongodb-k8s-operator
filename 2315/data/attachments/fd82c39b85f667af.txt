[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] active: 
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: backup started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] maintenance: restore started/running, backup id:'2024-09-07T01:52:42Z'
[32mINFO    [0m juju.model:model.py:2972 Waiting for model:
  config-server/0 [idle] active: 
  config-server/1 [idle] active: Primary
[32mINFO    [0m pytest_operator.plugin:plugin.py:834 Model status:

Model  Controller          Cloud/Region        Version  SLA          Timestamp
test   microk8s-localhost  microk8s/localhost  3.5.3    unsupported  02:01:49Z

App            Version  Status   Scale  Charm          Channel      Rev  Address         Exposed  Message
application             waiting      1  application                   0  10.152.183.130  no       installing agent
config-server           active       2  mongodb-k8s                   0  10.152.183.158  no       
s3-integrator           active       1  s3-integrator  latest/edge   41  10.152.183.156  no       
shard-one               active       2  mongodb-k8s                   1  10.152.183.182  no       Primary
shard-two               active       1  mongodb-k8s                   2  10.152.183.53   no       Primary

Unit              Workload  Agent  Address      Ports  Message
application/0*    waiting   idle   10.1.216.72         
config-server/0   active    idle   10.1.216.78         
config-server/1*  active    idle   10.1.216.77         Primary
s3-integrator/0*  active    idle   10.1.216.87         
shard-one/0*      active    idle   10.1.216.83         Primary
shard-one/1       active    idle   10.1.216.84         
shard-two/0*      active    idle   10.1.216.88         Primary

[32mINFO    [0m pytest_operator.plugin:plugin.py:840 Juju error logs:

unit-config-server-1: 01:41:27 ERROR unit.config-server/1.juju-log database-peers:1: Failed to create the operator user: non-zero exit code 1 executing ['/usr/bin/mongosh', 'mongodb://localhost/admin', '--quiet', '--eval', "db.createUser({  user: 'operator',  pwd: passwordPrompt(),  roles:[    {'role': 'userAdminAnyDatabase', 'db': 'admin'},     {'role': 'readWriteAnyDatabase', 'db': 'admin'},     {'role': 'clusterAdmin', 'db': 'admin'},   ],  mechanisms: ['SCRAM-SHA-256'],  passwordDigestor: 'server',})"], stdout='Enter password\n********************************', stderr='MongoServerError: not primary\n'
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-config-server-1/charm/./src/charm.py", line 939, in _init_operator_user
    process.wait_output()
  File "/var/lib/juju/agents/unit-config-server-1/charm/venv/ops/pebble.py", line 1773, in wait_output
    raise ExecError[AnyStr](self._command, exit_code, out_value, err_value)
ops.pebble.ExecError: non-zero exit code 1 executing ['/usr/bin/mongosh', 'mongodb://localhost/admin', '--quiet', '--eval', "db.createUser({  user: 'operator',  pwd: passwordPrompt(),  roles:[    {'role': 'userAdminAnyDatabase', 'db': 'admin'},     {'role': 'readWriteAnyDatabase', 'db': 'admin'},     {'role': 'clusterAdmin', 'db': 'admin'},   ],  mechanisms: ['SCRAM-SHA-256'],  passwordDigestor: 'server',})"], stdout='Enter password\n********************************', stderr='MongoServerError: not primary\n'
unit-config-server-1: 01:41:32 ERROR unit.config-server/1.juju-log database-peers:1: Failed to create the operator user: non-zero exit code 1 executing ['/usr/bin/mongosh', 'mongodb://localhost/admin', '--quiet', '--eval', "db.createUser({  user: 'operator',  pwd: passwordPrompt(),  roles:[    {'role': 'userAdminAnyDatabase', 'db': 'admin'},     {'role': 'readWriteAnyDatabase', 'db': 'admin'},     {'role': 'clusterAdmin', 'db': 'admin'},   ],  mechanisms: ['SCRAM-SHA-256'],  passwordDigestor: 'server',})"], stdout='Enter password\n********************************', stderr='MongoServerError: not primary\n'
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-config-server-1/charm/./src/charm.py", line 939, in _init_operator_user
    process.wait_output()
  File "/var/lib/juju/agents/unit-config-server-1/charm/venv/ops/pebble.py", line 1773, in wait_output
    raise ExecError[AnyStr](self._command, exit_code, out_value, err_value)
ops.pebble.ExecError: non-zero exit code 1 executing ['/usr/bin/mongosh', 'mongodb://localhost/admin', '--quiet', '--eval', "db.createUser({  user: 'operator',  pwd: passwordPrompt(),  roles:[    {'role': 'userAdminAnyDatabase', 'db': 'admin'},     {'role': 'readWriteAnyDatabase', 'db': 'admin'},     {'role': 'clusterAdmin', 'db': 'admin'},   ],  mechanisms: ['SCRAM-SHA-256'],  passwordDigestor: 'server',})"], stdout='Enter password\n********************************', stderr='MongoServerError: not primary\n'
unit-config-server-1: 01:42:41 ERROR unit.config-server/1.juju-log config-server:7: Failed to get pbm status: non-zero exit code 1 executing ['/usr/bin/pbm', 'status', '-o', 'json'], stdout='{"Error":"get status of pitr: unable check PITR config status: get config: missed config"}\n', stderr=''
unit-config-server-1: 01:42:44 ERROR unit.config-server/1.juju-log config-server:6: Failed to get pbm status: non-zero exit code 1 executing ['/usr/bin/pbm', 'status', '-o', 'json'], stdout='{"Error":"get status of pitr: unable check PITR config status: get config: missed config"}\n', stderr=''
unit-config-server-1: 01:42:45 ERROR unit.config-server/1.juju-log config-server:6: Failed to get pbm status: non-zero exit code 1 executing ['/usr/bin/pbm', 'status', '-o', 'json'], stdout='{"Error":"get status of pitr: unable check PITR config status: get config: missed config"}\n', stderr=''
unit-config-server-1: 01:42:48 ERROR unit.config-server/1.juju-log config-server:6: Failed to get pbm status: non-zero exit code 1 executing ['/usr/bin/pbm', 'status', '-o', 'json'], stdout='{"Error":"get status of pitr: unable check PITR config status: get config: missed config"}\n', stderr=''
unit-config-server-1: 01:43:03 ERROR unit.config-server/1.juju-log config-server:6: Failed to add shard shard-one to the config server, error=OperationFailure('Could not find host matching read preference { mode: "primary" } for set shard-one, full error: {\'ok\': 0.0, \'errmsg\': \'Could not find host matching read preference { mode: "primary" } for set shard-one\', \'code\': 133, \'codeName\': \'FailedToSatisfyReadPreference\', \'$clusterTime\': {\'clusterTime\': Timestamp(1725673383, 2), \'signature\': {\'hash\': b\'\\xfb"\\xc7\\xa9\\t\\xaa\\xe4e\\x13\\x89:\\x83\\x1f\\xa8\\x96\\x19\\xa3\\xba$G\', \'keyId\': 7411710369900527640}}, \'operationTime\': Timestamp(1725673383, 2)}')
unit-config-server-1: 01:43:03 ERROR unit.config-server/1.juju-log config-server:6: Failed to add shard-one to cluster
unit-config-server-1: 01:43:03 ERROR unit.config-server/1.juju-log config-server:6: Deferring _on_relation_event for shards interface since: error=OperationFailure('Could not find host matching read preference { mode: "primary" } for set shard-one, full error: {\'ok\': 0.0, \'errmsg\': \'Could not find host matching read preference { mode: "primary" } for set shard-one\', \'code\': 133, \'codeName\': \'FailedToSatisfyReadPreference\', \'$clusterTime\': {\'clusterTime\': Timestamp(1725673383, 2), \'signature\': {\'hash\': b\'\\xfb"\\xc7\\xa9\\t\\xaa\\xe4e\\x13\\x89:\\x83\\x1f\\xa8\\x96\\x19\\xa3\\xba$G\', \'keyId\': 7411710369900527640}}, \'operationTime\': Timestamp(1725673383, 2)}')
unit-config-server-1: 01:43:06 ERROR unit.config-server/1.juju-log s3-credentials:5: Failed to get pbm status: non-zero exit code 1 executing ['/usr/bin/pbm', 'status', '-o', 'json'], stdout='{"Error":"get status of cluster: connect to `shard-one` [shard-one/shard-one-0.shard-one-endpoints:27017,shard-one-1.shard-one-endpoints:27017]: ping: connection() error occurred during connection handshake: auth error: unable to authenticate using mechanism \\"SCRAM-SHA-256\\": (AuthenticationFailed) Authentication failed."}\n', stderr=''
unit-config-server-1: 01:43:16 ERROR unit.config-server/1.juju-log config-server:7: Failed to add shard shard-two to the config server, error=OperationFailure('Authentication failed., full error: {\'ok\': 0.0, \'errmsg\': \'Authentication failed.\', \'code\': 18, \'codeName\': \'AuthenticationFailed\', \'$clusterTime\': {\'clusterTime\': Timestamp(1725673396, 1), \'signature\': {\'hash\': b\'\\x90\\x07\\x93\\xea\\x0c-\\xca\\xde&;\\xeb\\xb9\\xd1\\xd2\\xb2"\\xa9\\x16\\x9b\\xdd\', \'keyId\': 7411710369900527640}}, \'operationTime\': Timestamp(1725673396, 1)}')
unit-config-server-1: 01:43:16 ERROR unit.config-server/1.juju-log config-server:7: shard-two shard does not have the same auth as the config server.
unit-shard-one-0: 01:47:57 ERROR unit.shard-one/0.juju-log Backup failed: Relation with s3-integrator charm missing, cannot restore from a backup.
unit-shard-one-0: 01:47:58 ERROR unit.shard-one/0.juju-log List-backups failed: Relation with s3-integrator charm missing, cannot restore from a backup.
unit-shard-one-0: 01:47:59 ERROR unit.shard-one/0.juju-log Restore failed: Relation with s3-integrator charm missing, cannot restore from a backup.

[32mINFO    [0m pytest_operator.plugin:plugin.py:904 Forgetting main...